# Waypoint VLM training config — LIBERO
# Usage: torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_waypoint.py --mode vlm --config configs/waypoint_vlm_libero.yaml

robot_type: libero

# Data paths
wp_rlds_dir: /workspace/data/libero/libero_object_wp_001/waypoint_filtered_rlds__libero/1.0.0
dataset_statistics_path: /workspace/data/libero_object_no_noops/1.0.0

# Model architecture
paligemma_variant: gemma_2b
action_expert_variant: gemma_300m  # Only needed for weight extraction
precision: bfloat16

# Waypoint config
num_waypoints: 7
max_token_len: 256
stride: 4

# Pretrained weights (Pi0.5 base — will extract PaliGemma portion)
pretrained_weight_path: /workspace/models/pi05_base_pytorch

# Training hyperparameters
batch_size: 12
num_train_steps: 30000
warmup_steps: 500
peak_lr: 5.0e-5
end_lr: 1.0e-7

# Normalization
norm_type: q99

# Shuffle
shuffle_buffer_size: 5000

# Logging and checkpointing
exp_name: waypoint_vlm_libero_new1
checkpoint_dir: checkpoints/waypoint_vlm_libero_new1
save_interval: 50
log_interval: 10
wandb_enabled: true

# LoRA (set to true for low-memory training)
lora_enabled: false
lora_rank: 16
lora_alpha: 16.0
train_vision_encoder: true

# Resume from checkpoint
resume: false
