# Waypoint Action Expert training config — LIBERO
# Usage: torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_waypoint.py --mode ae --config configs/waypoint_ae_libero.yaml

robot_type: libero

# Data paths
original_rlds_dir: /workspace/data/libero/libero_object_no_noops/libero_object_no_noops/1.0.0
wp_indices_path: /workspace/data/libero/libero_object_wp_001/waypoint_indices.json
dataset_statistics_path: /workspace/data/libero_object_no_noops/1.0.0

# Model dimensions
model_action_dim: 32
model_proprio_dim: 32
horizon_steps: 32
max_duration: 32
max_token_len: 64

# Model architecture
paligemma_variant: gemma_2b
action_expert_variant: gemma_300m
precision: bfloat16

# Pretrained weights (Pi0.5 base)
pretrained_weight_path: /workspace/openpi/checkpoints/waypoint_ae_libero/2400

# Training hyperparameters
# batch_size is per GPU (each rank loads independently via IterableDataset DDP split).
# With 2 GPUs: effective total batch = batch_size * 2.
# Memory profiling on 2x RTX PRO 6000 Blackwell (97.9 GB each):
#   batch=32  → 59.1 GB / 50.1 GB  (38.8 / 47.8 GB free)  ← ~261 MiB/sample (measured)
#   batch=48  → 63.3 GB / 54.3 GB  (34.6 / 43.6 GB free)
#   batch=128 → 75.2 GB / 66.1 GB  (22.1 / 31.1 GB free)  ← ~148 MiB/sample (measured)
# Memory profiling (all values measured after step=1 forward+backward+optimizer):
#   batch=32  → 59.1 GB / 50.1 GB  (38.8 / 47.8 GB free)
#   batch=48  → 63.3 GB / 54.3 GB  (34.6 / 43.6 GB free)
#   batch=128 → 75.2 GB / 66.1 GB  (22.1 / 31.1 GB free)
#   batch=144 → 78.7 GB / 69.7 GB  (18.5 / 27.6 GB free)  ← SAFE MAXIMUM
#   batch=160 → 97.1 GB / 89.6 GB  (0.1 / 7.6 GB free!)   ← OOM IMMINENT, DO NOT USE
#   batch=192 → 95.9 GB / 86.8 GB  (1.3 GB free!)         ← OOM RISK, DO NOT USE
# NOTE: batch=144→160 causes nonlinear memory jump (~1149 MiB/sample vs ~223 at 128→144)
#       likely due to DDP gradient sync buffer threshold being crossed
batch_size: 144
num_train_steps: 2500    # 144×2=288 effective batch; 2.5k steps ≈ 720K samples seen
warmup_steps: 20
peak_lr: 1.0e-5           # sqrt LR scaling: 5e-5 × sqrt(288/64) ≈ 1.06e-4, conservative 9e-5
end_lr: 1.0e-8

# Normalization
norm_type: q99

# Image augmentation — applied in preprocessing_pytorch.py (train=True).
# Remove or comment out image_aug_cfg to use the default values shown below.
image_aug_cfg:
  crop_scale: 0.95        # non-wrist random crop scale (default: 0.95)
  rotation_deg: 5.0       # non-wrist max rotation in degrees (default: 5.0)
  brightness_lo: 0.7      # brightness factor range lo (default: 0.7)
  brightness_hi: 1.3      # brightness factor range hi (default: 1.3)
  contrast_lo: 0.6        # contrast factor range lo (default: 0.6)
  contrast_hi: 1.4        # contrast factor range hi (default: 1.4)
  saturation_lo: 0.5      # saturation factor range lo (default: 0.5)
  saturation_hi: 1.5      # saturation factor range hi (default: 1.5)

# Shuffle
shuffle_buffer_size: 1000

# Logging and checkpointing
exp_name: waypoint_ae_libero_01
checkpoint_dir: checkpoints/{exp_name}
save_interval: 150
log_interval: 5
wandb_enabled: true

# LoRA (set to true for low-memory training)
lora_enabled: false
lora_rank: 16
lora_alpha: 16.0
train_vision_encoder: true

# Resume from checkpoint
resume: false
